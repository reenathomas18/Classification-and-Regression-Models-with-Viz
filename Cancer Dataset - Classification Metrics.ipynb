{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION METRICS\n",
    "Given cancer dataset to predict if the patient has benign or malignant tumour using the below techniques.\n",
    "\n",
    "a. Numeric techniques \n",
    "\n",
    "b. Visualization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Numeric Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading DataSet\n",
    "df = pd.read_csv('cancer.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>87139402</td>\n",
       "      <td>B</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>78.85</td>\n",
       "      <td>464.1</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>15.64</td>\n",
       "      <td>86.97</td>\n",
       "      <td>549.1</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.12420</td>\n",
       "      <td>0.09391</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8910251</td>\n",
       "      <td>B</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.95</td>\n",
       "      <td>69.28</td>\n",
       "      <td>346.4</td>\n",
       "      <td>0.09688</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.06387</td>\n",
       "      <td>0.02642</td>\n",
       "      <td>...</td>\n",
       "      <td>11.88</td>\n",
       "      <td>22.94</td>\n",
       "      <td>78.28</td>\n",
       "      <td>424.8</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.19160</td>\n",
       "      <td>0.07926</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.07587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>905520</td>\n",
       "      <td>B</td>\n",
       "      <td>11.04</td>\n",
       "      <td>16.83</td>\n",
       "      <td>70.92</td>\n",
       "      <td>373.2</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.02480</td>\n",
       "      <td>...</td>\n",
       "      <td>12.41</td>\n",
       "      <td>26.44</td>\n",
       "      <td>79.93</td>\n",
       "      <td>471.4</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.10670</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2998</td>\n",
       "      <td>0.07881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>868871</td>\n",
       "      <td>B</td>\n",
       "      <td>11.28</td>\n",
       "      <td>13.39</td>\n",
       "      <td>73.00</td>\n",
       "      <td>384.8</td>\n",
       "      <td>0.11640</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.04635</td>\n",
       "      <td>0.04796</td>\n",
       "      <td>...</td>\n",
       "      <td>11.92</td>\n",
       "      <td>15.77</td>\n",
       "      <td>76.53</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.08669</td>\n",
       "      <td>0.08611</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.06784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9012568</td>\n",
       "      <td>B</td>\n",
       "      <td>15.19</td>\n",
       "      <td>13.21</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.8</td>\n",
       "      <td>0.07963</td>\n",
       "      <td>0.06934</td>\n",
       "      <td>0.03393</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>...</td>\n",
       "      <td>16.20</td>\n",
       "      <td>15.73</td>\n",
       "      <td>104.50</td>\n",
       "      <td>819.1</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.13620</td>\n",
       "      <td>0.08178</td>\n",
       "      <td>0.2487</td>\n",
       "      <td>0.06766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0  87139402         B        12.32         12.39           78.85      464.1   \n",
       "1   8910251         B        10.60         18.95           69.28      346.4   \n",
       "2    905520         B        11.04         16.83           70.92      373.2   \n",
       "3    868871         B        11.28         13.39           73.00      384.8   \n",
       "4   9012568         B        15.19         13.21           97.65      711.8   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  points_mean  ...  \\\n",
       "0          0.10280           0.06981         0.03987      0.03700  ...   \n",
       "1          0.09688           0.11470         0.06387      0.02642  ...   \n",
       "2          0.10770           0.07804         0.03046      0.02480  ...   \n",
       "3          0.11640           0.11360         0.04635      0.04796  ...   \n",
       "4          0.07963           0.06934         0.03393      0.02657  ...   \n",
       "\n",
       "   radius_worst  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0         13.50          15.64            86.97       549.1            0.1385   \n",
       "1         11.88          22.94            78.28       424.8            0.1213   \n",
       "2         12.41          26.44            79.93       471.4            0.1369   \n",
       "3         11.92          15.77            76.53       434.0            0.1367   \n",
       "4         16.20          15.73           104.50       819.1            0.1126   \n",
       "\n",
       "   compactness_worst  concavity_worst  points_worst  symmetry_worst  \\\n",
       "0             0.1266          0.12420       0.09391          0.2827   \n",
       "1             0.2515          0.19160       0.07926          0.2940   \n",
       "2             0.1482          0.10670       0.07431          0.2998   \n",
       "3             0.1822          0.08669       0.08611          0.2102   \n",
       "4             0.1737          0.13620       0.08178          0.2487   \n",
       "\n",
       "   dimension_worst  \n",
       "0          0.06771  \n",
       "1          0.07587  \n",
       "2          0.07881  \n",
       "3          0.06784  \n",
       "4          0.06766  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>3.037183e+07</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.250206e+08</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>8.670000e+03</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>8.692180e+05</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>9.060240e+05</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>8.813129e+06</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9.113205e+08</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \n",
       "mean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \n",
       "std    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \n",
       "min    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n",
       "25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n",
       "50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n",
       "75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \n",
       "max    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  points_mean  \\\n",
       "count       569.000000        569.000000      569.000000   569.000000   \n",
       "mean          0.096360          0.104341        0.088799     0.048919   \n",
       "std           0.014064          0.052813        0.079720     0.038803   \n",
       "min           0.052630          0.019380        0.000000     0.000000   \n",
       "25%           0.086370          0.064920        0.029560     0.020310   \n",
       "50%           0.095870          0.092630        0.061540     0.033500   \n",
       "75%           0.105300          0.130400        0.130700     0.074000   \n",
       "max           0.163400          0.345400        0.426800     0.201200   \n",
       "\n",
       "       symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "count     569.000000  ...    569.000000     569.000000       569.000000   \n",
       "mean        0.181162  ...     16.269190      25.677223       107.261213   \n",
       "std         0.027414  ...      4.833242       6.146258        33.602542   \n",
       "min         0.106000  ...      7.930000      12.020000        50.410000   \n",
       "25%         0.161900  ...     13.010000      21.080000        84.110000   \n",
       "50%         0.179200  ...     14.970000      25.410000        97.660000   \n",
       "75%         0.195700  ...     18.790000      29.720000       125.400000   \n",
       "max         0.304000  ...     36.040000      49.540000       251.200000   \n",
       "\n",
       "        area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "count   569.000000        569.000000         569.000000       569.000000   \n",
       "mean    880.583128          0.132369           0.254265         0.272188   \n",
       "std     569.356993          0.022832           0.157336         0.208624   \n",
       "min     185.200000          0.071170           0.027290         0.000000   \n",
       "25%     515.300000          0.116600           0.147200         0.114500   \n",
       "50%     686.500000          0.131300           0.211900         0.226700   \n",
       "75%    1084.000000          0.146000           0.339100         0.382900   \n",
       "max    4254.000000          0.222600           1.058000         1.252000   \n",
       "\n",
       "       points_worst  symmetry_worst  dimension_worst  \n",
       "count    569.000000      569.000000       569.000000  \n",
       "mean       0.114606        0.290076         0.083946  \n",
       "std        0.065732        0.061867         0.018061  \n",
       "min        0.000000        0.156500         0.055040  \n",
       "25%        0.064930        0.250400         0.071460  \n",
       "50%        0.099930        0.282200         0.080040  \n",
       "75%        0.161400        0.317900         0.092080  \n",
       "max        0.291000        0.663800         0.207500  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diagnosis.replace(['B','M'],[0,1] , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>87139402</td>\n",
       "      <td>0</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>78.85</td>\n",
       "      <td>464.1</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>15.64</td>\n",
       "      <td>86.97</td>\n",
       "      <td>549.1</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.12420</td>\n",
       "      <td>0.09391</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8910251</td>\n",
       "      <td>0</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.95</td>\n",
       "      <td>69.28</td>\n",
       "      <td>346.4</td>\n",
       "      <td>0.09688</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.06387</td>\n",
       "      <td>0.02642</td>\n",
       "      <td>...</td>\n",
       "      <td>11.88</td>\n",
       "      <td>22.94</td>\n",
       "      <td>78.28</td>\n",
       "      <td>424.8</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.19160</td>\n",
       "      <td>0.07926</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.07587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>905520</td>\n",
       "      <td>0</td>\n",
       "      <td>11.04</td>\n",
       "      <td>16.83</td>\n",
       "      <td>70.92</td>\n",
       "      <td>373.2</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.02480</td>\n",
       "      <td>...</td>\n",
       "      <td>12.41</td>\n",
       "      <td>26.44</td>\n",
       "      <td>79.93</td>\n",
       "      <td>471.4</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.10670</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2998</td>\n",
       "      <td>0.07881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>868871</td>\n",
       "      <td>0</td>\n",
       "      <td>11.28</td>\n",
       "      <td>13.39</td>\n",
       "      <td>73.00</td>\n",
       "      <td>384.8</td>\n",
       "      <td>0.11640</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.04635</td>\n",
       "      <td>0.04796</td>\n",
       "      <td>...</td>\n",
       "      <td>11.92</td>\n",
       "      <td>15.77</td>\n",
       "      <td>76.53</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.08669</td>\n",
       "      <td>0.08611</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.06784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9012568</td>\n",
       "      <td>0</td>\n",
       "      <td>15.19</td>\n",
       "      <td>13.21</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.8</td>\n",
       "      <td>0.07963</td>\n",
       "      <td>0.06934</td>\n",
       "      <td>0.03393</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>...</td>\n",
       "      <td>16.20</td>\n",
       "      <td>15.73</td>\n",
       "      <td>104.50</td>\n",
       "      <td>819.1</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.13620</td>\n",
       "      <td>0.08178</td>\n",
       "      <td>0.2487</td>\n",
       "      <td>0.06766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>911320502</td>\n",
       "      <td>0</td>\n",
       "      <td>13.17</td>\n",
       "      <td>18.22</td>\n",
       "      <td>84.28</td>\n",
       "      <td>537.3</td>\n",
       "      <td>0.07466</td>\n",
       "      <td>0.05994</td>\n",
       "      <td>0.04859</td>\n",
       "      <td>0.02870</td>\n",
       "      <td>...</td>\n",
       "      <td>14.90</td>\n",
       "      <td>23.89</td>\n",
       "      <td>95.10</td>\n",
       "      <td>687.6</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.1965</td>\n",
       "      <td>0.18760</td>\n",
       "      <td>0.10450</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.06925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>898677</td>\n",
       "      <td>0</td>\n",
       "      <td>10.26</td>\n",
       "      <td>14.71</td>\n",
       "      <td>66.20</td>\n",
       "      <td>321.6</td>\n",
       "      <td>0.09882</td>\n",
       "      <td>0.09159</td>\n",
       "      <td>0.03581</td>\n",
       "      <td>0.02037</td>\n",
       "      <td>...</td>\n",
       "      <td>10.88</td>\n",
       "      <td>19.48</td>\n",
       "      <td>70.89</td>\n",
       "      <td>357.1</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.1636</td>\n",
       "      <td>0.07162</td>\n",
       "      <td>0.04074</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>0.08488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>873885</td>\n",
       "      <td>1</td>\n",
       "      <td>15.28</td>\n",
       "      <td>22.41</td>\n",
       "      <td>98.92</td>\n",
       "      <td>710.6</td>\n",
       "      <td>0.09057</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.05375</td>\n",
       "      <td>0.03263</td>\n",
       "      <td>...</td>\n",
       "      <td>17.80</td>\n",
       "      <td>28.03</td>\n",
       "      <td>113.80</td>\n",
       "      <td>973.1</td>\n",
       "      <td>0.1301</td>\n",
       "      <td>0.3299</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.09772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>911201</td>\n",
       "      <td>0</td>\n",
       "      <td>14.53</td>\n",
       "      <td>13.98</td>\n",
       "      <td>93.86</td>\n",
       "      <td>644.2</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.09242</td>\n",
       "      <td>0.06895</td>\n",
       "      <td>0.06495</td>\n",
       "      <td>...</td>\n",
       "      <td>15.80</td>\n",
       "      <td>16.93</td>\n",
       "      <td>103.10</td>\n",
       "      <td>749.9</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>0.1478</td>\n",
       "      <td>0.13730</td>\n",
       "      <td>0.10690</td>\n",
       "      <td>0.2606</td>\n",
       "      <td>0.07810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>9012795</td>\n",
       "      <td>1</td>\n",
       "      <td>21.37</td>\n",
       "      <td>15.10</td>\n",
       "      <td>141.30</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.15150</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>...</td>\n",
       "      <td>22.69</td>\n",
       "      <td>21.84</td>\n",
       "      <td>152.10</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>0.1192</td>\n",
       "      <td>0.2840</td>\n",
       "      <td>0.40240</td>\n",
       "      <td>0.19660</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>0.08666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n",
       "0     87139402          0        12.32         12.39           78.85   \n",
       "1      8910251          0        10.60         18.95           69.28   \n",
       "2       905520          0        11.04         16.83           70.92   \n",
       "3       868871          0        11.28         13.39           73.00   \n",
       "4      9012568          0        15.19         13.21           97.65   \n",
       "..         ...        ...          ...           ...             ...   \n",
       "564  911320502          0        13.17         18.22           84.28   \n",
       "565     898677          0        10.26         14.71           66.20   \n",
       "566     873885          1        15.28         22.41           98.92   \n",
       "567     911201          0        14.53         13.98           93.86   \n",
       "568    9012795          1        21.37         15.10          141.30   \n",
       "\n",
       "     area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "0        464.1          0.10280           0.06981         0.03987   \n",
       "1        346.4          0.09688           0.11470         0.06387   \n",
       "2        373.2          0.10770           0.07804         0.03046   \n",
       "3        384.8          0.11640           0.11360         0.04635   \n",
       "4        711.8          0.07963           0.06934         0.03393   \n",
       "..         ...              ...               ...             ...   \n",
       "564      537.3          0.07466           0.05994         0.04859   \n",
       "565      321.6          0.09882           0.09159         0.03581   \n",
       "566      710.6          0.09057           0.10520         0.05375   \n",
       "567      644.2          0.10990           0.09242         0.06895   \n",
       "568     1386.0          0.10010           0.15150         0.19320   \n",
       "\n",
       "     points_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0        0.03700  ...         13.50          15.64            86.97   \n",
       "1        0.02642  ...         11.88          22.94            78.28   \n",
       "2        0.02480  ...         12.41          26.44            79.93   \n",
       "3        0.04796  ...         11.92          15.77            76.53   \n",
       "4        0.02657  ...         16.20          15.73           104.50   \n",
       "..           ...  ...           ...            ...              ...   \n",
       "564      0.02870  ...         14.90          23.89            95.10   \n",
       "565      0.02037  ...         10.88          19.48            70.89   \n",
       "566      0.03263  ...         17.80          28.03           113.80   \n",
       "567      0.06495  ...         15.80          16.93           103.10   \n",
       "568      0.12550  ...         22.69          21.84           152.10   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0         549.1            0.1385             0.1266          0.12420   \n",
       "1         424.8            0.1213             0.2515          0.19160   \n",
       "2         471.4            0.1369             0.1482          0.10670   \n",
       "3         434.0            0.1367             0.1822          0.08669   \n",
       "4         819.1            0.1126             0.1737          0.13620   \n",
       "..          ...               ...                ...              ...   \n",
       "564       687.6            0.1282             0.1965          0.18760   \n",
       "565       357.1            0.1360             0.1636          0.07162   \n",
       "566       973.1            0.1301             0.3299          0.36300   \n",
       "567       749.9            0.1347             0.1478          0.13730   \n",
       "568      1535.0            0.1192             0.2840          0.40240   \n",
       "\n",
       "     points_worst  symmetry_worst  dimension_worst  \n",
       "0         0.09391          0.2827          0.06771  \n",
       "1         0.07926          0.2940          0.07587  \n",
       "2         0.07431          0.2998          0.07881  \n",
       "3         0.08611          0.2102          0.06784  \n",
       "4         0.08178          0.2487          0.06766  \n",
       "..            ...             ...              ...  \n",
       "564       0.10450          0.2235          0.06925  \n",
       "565       0.04074          0.2434          0.08488  \n",
       "566       0.12260          0.3175          0.09772  \n",
       "567       0.10690          0.2606          0.07810  \n",
       "568       0.19660          0.2730          0.08666  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check whether teh data is a balanced dataset or not\n",
    "df.diagnosis.value_counts()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules for Classification Usecase if you use SKLEARN for modelling\n",
    "# 1. Data must be complete\n",
    "# 2. Features must be strictly numeric. Labels can be numeric or non-numeric\n",
    "# 3. Data must be represented in the form of numpy array\n",
    "# 4. Features must be a 2d array\n",
    "# 5. Label must be  1d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>78.85</td>\n",
       "      <td>464.1</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.1959</td>\n",
       "      <td>0.05955</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>15.64</td>\n",
       "      <td>86.97</td>\n",
       "      <td>549.1</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.12420</td>\n",
       "      <td>0.09391</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.95</td>\n",
       "      <td>69.28</td>\n",
       "      <td>346.4</td>\n",
       "      <td>0.09688</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.06387</td>\n",
       "      <td>0.02642</td>\n",
       "      <td>0.1922</td>\n",
       "      <td>0.06491</td>\n",
       "      <td>...</td>\n",
       "      <td>11.88</td>\n",
       "      <td>22.94</td>\n",
       "      <td>78.28</td>\n",
       "      <td>424.8</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.19160</td>\n",
       "      <td>0.07926</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.07587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.04</td>\n",
       "      <td>16.83</td>\n",
       "      <td>70.92</td>\n",
       "      <td>373.2</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.02480</td>\n",
       "      <td>0.1714</td>\n",
       "      <td>0.06340</td>\n",
       "      <td>...</td>\n",
       "      <td>12.41</td>\n",
       "      <td>26.44</td>\n",
       "      <td>79.93</td>\n",
       "      <td>471.4</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.10670</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2998</td>\n",
       "      <td>0.07881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.28</td>\n",
       "      <td>13.39</td>\n",
       "      <td>73.00</td>\n",
       "      <td>384.8</td>\n",
       "      <td>0.11640</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.04635</td>\n",
       "      <td>0.04796</td>\n",
       "      <td>0.1771</td>\n",
       "      <td>0.06072</td>\n",
       "      <td>...</td>\n",
       "      <td>11.92</td>\n",
       "      <td>15.77</td>\n",
       "      <td>76.53</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.08669</td>\n",
       "      <td>0.08611</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.06784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>15.19</td>\n",
       "      <td>13.21</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.8</td>\n",
       "      <td>0.07963</td>\n",
       "      <td>0.06934</td>\n",
       "      <td>0.03393</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>0.1721</td>\n",
       "      <td>0.05544</td>\n",
       "      <td>...</td>\n",
       "      <td>16.20</td>\n",
       "      <td>15.73</td>\n",
       "      <td>104.50</td>\n",
       "      <td>819.1</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.13620</td>\n",
       "      <td>0.08178</td>\n",
       "      <td>0.2487</td>\n",
       "      <td>0.06766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>13.17</td>\n",
       "      <td>18.22</td>\n",
       "      <td>84.28</td>\n",
       "      <td>537.3</td>\n",
       "      <td>0.07466</td>\n",
       "      <td>0.05994</td>\n",
       "      <td>0.04859</td>\n",
       "      <td>0.02870</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.05549</td>\n",
       "      <td>...</td>\n",
       "      <td>14.90</td>\n",
       "      <td>23.89</td>\n",
       "      <td>95.10</td>\n",
       "      <td>687.6</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.1965</td>\n",
       "      <td>0.18760</td>\n",
       "      <td>0.10450</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.06925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>10.26</td>\n",
       "      <td>14.71</td>\n",
       "      <td>66.20</td>\n",
       "      <td>321.6</td>\n",
       "      <td>0.09882</td>\n",
       "      <td>0.09159</td>\n",
       "      <td>0.03581</td>\n",
       "      <td>0.02037</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.07005</td>\n",
       "      <td>...</td>\n",
       "      <td>10.88</td>\n",
       "      <td>19.48</td>\n",
       "      <td>70.89</td>\n",
       "      <td>357.1</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>0.1636</td>\n",
       "      <td>0.07162</td>\n",
       "      <td>0.04074</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>0.08488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>15.28</td>\n",
       "      <td>22.41</td>\n",
       "      <td>98.92</td>\n",
       "      <td>710.6</td>\n",
       "      <td>0.09057</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.05375</td>\n",
       "      <td>0.03263</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>0.06317</td>\n",
       "      <td>...</td>\n",
       "      <td>17.80</td>\n",
       "      <td>28.03</td>\n",
       "      <td>113.80</td>\n",
       "      <td>973.1</td>\n",
       "      <td>0.1301</td>\n",
       "      <td>0.3299</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.09772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>14.53</td>\n",
       "      <td>13.98</td>\n",
       "      <td>93.86</td>\n",
       "      <td>644.2</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.09242</td>\n",
       "      <td>0.06895</td>\n",
       "      <td>0.06495</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.06121</td>\n",
       "      <td>...</td>\n",
       "      <td>15.80</td>\n",
       "      <td>16.93</td>\n",
       "      <td>103.10</td>\n",
       "      <td>749.9</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>0.1478</td>\n",
       "      <td>0.13730</td>\n",
       "      <td>0.10690</td>\n",
       "      <td>0.2606</td>\n",
       "      <td>0.07810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>21.37</td>\n",
       "      <td>15.10</td>\n",
       "      <td>141.30</td>\n",
       "      <td>1386.0</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.15150</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.1973</td>\n",
       "      <td>0.06183</td>\n",
       "      <td>...</td>\n",
       "      <td>22.69</td>\n",
       "      <td>21.84</td>\n",
       "      <td>152.10</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>0.1192</td>\n",
       "      <td>0.2840</td>\n",
       "      <td>0.40240</td>\n",
       "      <td>0.19660</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>0.08666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0          12.32         12.39           78.85      464.1          0.10280   \n",
       "1          10.60         18.95           69.28      346.4          0.09688   \n",
       "2          11.04         16.83           70.92      373.2          0.10770   \n",
       "3          11.28         13.39           73.00      384.8          0.11640   \n",
       "4          15.19         13.21           97.65      711.8          0.07963   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        13.17         18.22           84.28      537.3          0.07466   \n",
       "565        10.26         14.71           66.20      321.6          0.09882   \n",
       "566        15.28         22.41           98.92      710.6          0.09057   \n",
       "567        14.53         13.98           93.86      644.2          0.10990   \n",
       "568        21.37         15.10          141.30     1386.0          0.10010   \n",
       "\n",
       "     compactness_mean  concavity_mean  points_mean  symmetry_mean  \\\n",
       "0             0.06981         0.03987      0.03700         0.1959   \n",
       "1             0.11470         0.06387      0.02642         0.1922   \n",
       "2             0.07804         0.03046      0.02480         0.1714   \n",
       "3             0.11360         0.04635      0.04796         0.1771   \n",
       "4             0.06934         0.03393      0.02657         0.1721   \n",
       "..                ...             ...          ...            ...   \n",
       "564           0.05994         0.04859      0.02870         0.1454   \n",
       "565           0.09159         0.03581      0.02037         0.1633   \n",
       "566           0.10520         0.05375      0.03263         0.1727   \n",
       "567           0.09242         0.06895      0.06495         0.1650   \n",
       "568           0.15150         0.19320      0.12550         0.1973   \n",
       "\n",
       "     dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.05955  ...         13.50          15.64            86.97   \n",
       "1           0.06491  ...         11.88          22.94            78.28   \n",
       "2           0.06340  ...         12.41          26.44            79.93   \n",
       "3           0.06072  ...         11.92          15.77            76.53   \n",
       "4           0.05544  ...         16.20          15.73           104.50   \n",
       "..              ...  ...           ...            ...              ...   \n",
       "564         0.05549  ...         14.90          23.89            95.10   \n",
       "565         0.07005  ...         10.88          19.48            70.89   \n",
       "566         0.06317  ...         17.80          28.03           113.80   \n",
       "567         0.06121  ...         15.80          16.93           103.10   \n",
       "568         0.06183  ...         22.69          21.84           152.10   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0         549.1            0.1385             0.1266          0.12420   \n",
       "1         424.8            0.1213             0.2515          0.19160   \n",
       "2         471.4            0.1369             0.1482          0.10670   \n",
       "3         434.0            0.1367             0.1822          0.08669   \n",
       "4         819.1            0.1126             0.1737          0.13620   \n",
       "..          ...               ...                ...              ...   \n",
       "564       687.6            0.1282             0.1965          0.18760   \n",
       "565       357.1            0.1360             0.1636          0.07162   \n",
       "566       973.1            0.1301             0.3299          0.36300   \n",
       "567       749.9            0.1347             0.1478          0.13730   \n",
       "568      1535.0            0.1192             0.2840          0.40240   \n",
       "\n",
       "     points_worst  symmetry_worst  dimension_worst  \n",
       "0         0.09391          0.2827          0.06771  \n",
       "1         0.07926          0.2940          0.07587  \n",
       "2         0.07431          0.2998          0.07881  \n",
       "3         0.08611          0.2102          0.06784  \n",
       "4         0.08178          0.2487          0.06766  \n",
       "..            ...             ...              ...  \n",
       "564       0.10450          0.2235          0.06925  \n",
       "565       0.04074          0.2434          0.08488  \n",
       "566       0.12260          0.3175          0.09772  \n",
       "567       0.10690          0.2606          0.07810  \n",
       "568       0.19660          0.2730          0.08666  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seperate data as features and label\n",
    "features = df.iloc[:,2:33]\n",
    "label = df.diagnosis.values\n",
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.9824561403508771 Train: 0.9538461538461539 RS: 1\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 2\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 9\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 14\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 15\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 16\n",
      "Test: 0.956140350877193 Train: 0.9494505494505494 RS: 19\n",
      "Test: 0.9736842105263158 Train: 0.9494505494505494 RS: 21\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 22\n",
      "Test: 0.9649122807017544 Train: 0.9582417582417583 RS: 25\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 32\n",
      "Test: 0.9736842105263158 Train: 0.9494505494505494 RS: 33\n",
      "Test: 0.9824561403508771 Train: 0.9538461538461539 RS: 35\n",
      "Test: 0.9649122807017544 Train: 0.9494505494505494 RS: 36\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 40\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 42\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 43\n",
      "Test: 0.9824561403508771 Train: 0.9516483516483516 RS: 45\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 50\n",
      "Test: 0.9824561403508771 Train: 0.945054945054945 RS: 51\n",
      "Test: 0.9824561403508771 Train: 0.9472527472527472 RS: 54\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 56\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 57\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 58\n",
      "Test: 0.9912280701754386 Train: 0.9494505494505494 RS: 59\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 63\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 64\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 65\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 66\n",
      "Test: 0.9649122807017544 Train: 0.9494505494505494 RS: 68\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 69\n",
      "Test: 0.9824561403508771 Train: 0.9472527472527472 RS: 70\n",
      "Test: 0.9736842105263158 Train: 0.9472527472527472 RS: 72\n",
      "Test: 0.9649122807017544 Train: 0.9494505494505494 RS: 74\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 77\n",
      "Test: 0.9824561403508771 Train: 0.9472527472527472 RS: 79\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 80\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 81\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 82\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 87\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 88\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 91\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 92\n",
      "Test: 0.9824561403508771 Train: 0.9538461538461539 RS: 95\n",
      "Test: 0.956140350877193 Train: 0.9516483516483516 RS: 96\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 98\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 104\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 105\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 107\n",
      "Test: 0.9649122807017544 Train: 0.9494505494505494 RS: 110\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 112\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 114\n",
      "Test: 0.9736842105263158 Train: 0.9560439560439561 RS: 116\n",
      "Test: 0.9824561403508771 Train: 0.9472527472527472 RS: 118\n",
      "Test: 0.9649122807017544 Train: 0.9604395604395605 RS: 119\n",
      "Test: 0.9736842105263158 Train: 0.9582417582417583 RS: 123\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 126\n",
      "Test: 0.9824561403508771 Train: 0.9472527472527472 RS: 130\n",
      "Test: 0.9912280701754386 Train: 0.9494505494505494 RS: 136\n",
      "Test: 0.9649122807017544 Train: 0.9604395604395605 RS: 142\n",
      "Test: 0.9649122807017544 Train: 0.9582417582417583 RS: 143\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 145\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 146\n",
      "Test: 0.9736842105263158 Train: 0.9494505494505494 RS: 149\n",
      "Test: 0.9736842105263158 Train: 0.9494505494505494 RS: 150\n",
      "Test: 0.9736842105263158 Train: 0.9582417582417583 RS: 151\n",
      "Test: 0.9736842105263158 Train: 0.9560439560439561 RS: 154\n",
      "Test: 1.0 Train: 0.9494505494505494 RS: 155\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 160\n",
      "Test: 0.956140350877193 Train: 0.9472527472527472 RS: 161\n",
      "Test: 0.9649122807017544 Train: 0.9582417582417583 RS: 162\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 166\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 167\n",
      "Test: 0.9736842105263158 Train: 0.9494505494505494 RS: 168\n",
      "Test: 0.956140350877193 Train: 0.9516483516483516 RS: 171\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 180\n",
      "Test: 0.9824561403508771 Train: 0.9538461538461539 RS: 181\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 184\n",
      "Test: 0.9736842105263158 Train: 0.9494505494505494 RS: 187\n",
      "Test: 0.9736842105263158 Train: 0.9604395604395605 RS: 189\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 192\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 195\n",
      "Test: 0.9736842105263158 Train: 0.9472527472527472 RS: 196\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 198\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 204\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 206\n",
      "Test: 0.9736842105263158 Train: 0.9582417582417583 RS: 211\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 212\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 213\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 215\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 216\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 217\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 220\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 224\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 230\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 233\n",
      "Test: 0.9649122807017544 Train: 0.945054945054945 RS: 247\n",
      "Test: 0.9824561403508771 Train: 0.9472527472527472 RS: 249\n",
      "Test: 0.9824561403508771 Train: 0.9516483516483516 RS: 250\n",
      "Test: 0.9824561403508771 Train: 0.9494505494505494 RS: 253\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 257\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 260\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 263\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 265\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 266\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 270\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 272\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 273\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 274\n",
      "Test: 0.9824561403508771 Train: 0.9472527472527472 RS: 277\n",
      "Test: 0.956140350877193 Train: 0.9516483516483516 RS: 278\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 285\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 298\n",
      "Test: 0.9736842105263158 Train: 0.9560439560439561 RS: 300\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 301\n",
      "Test: 0.9824561403508771 Train: 0.9494505494505494 RS: 305\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 310\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 314\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 315\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 317\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 318\n",
      "Test: 0.9649122807017544 Train: 0.9648351648351648 RS: 319\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 320\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 321\n",
      "Test: 0.9736842105263158 Train: 0.945054945054945 RS: 322\n",
      "Test: 0.9824561403508771 Train: 0.9538461538461539 RS: 330\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 332\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 333\n",
      "Test: 0.9824561403508771 Train: 0.9538461538461539 RS: 334\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 335\n",
      "Test: 0.9649122807017544 Train: 0.9582417582417583 RS: 339\n",
      "Test: 0.9649122807017544 Train: 0.9604395604395605 RS: 343\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 345\n",
      "Test: 0.9824561403508771 Train: 0.9472527472527472 RS: 346\n",
      "Test: 0.9824561403508771 Train: 0.9538461538461539 RS: 351\n",
      "Test: 0.9824561403508771 Train: 0.9560439560439561 RS: 353\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 354\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 361\n",
      "Test: 0.9649122807017544 Train: 0.9494505494505494 RS: 362\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 363\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 364\n",
      "Test: 0.9912280701754386 Train: 0.9516483516483516 RS: 366\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 368\n",
      "Test: 0.956140350877193 Train: 0.9494505494505494 RS: 369\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 372\n",
      "Test: 0.9824561403508771 Train: 0.9494505494505494 RS: 376\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 378\n",
      "Test: 0.9736842105263158 Train: 0.9582417582417583 RS: 380\n",
      "Test: 0.9736842105263158 Train: 0.9560439560439561 RS: 385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 392\n",
      "Test: 0.9649122807017544 Train: 0.9494505494505494 RS: 394\n",
      "Test: 0.9736842105263158 Train: 0.9472527472527472 RS: 401\n",
      "Test: 0.9649122807017544 Train: 0.9582417582417583 RS: 403\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 406\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 408\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 409\n",
      "Test: 0.9736842105263158 Train: 0.9560439560439561 RS: 412\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 413\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 415\n",
      "Test: 0.9649122807017544 Train: 0.9604395604395605 RS: 416\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 418\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 422\n",
      "Test: 0.9912280701754386 Train: 0.945054945054945 RS: 424\n",
      "Test: 0.9824561403508771 Train: 0.9582417582417583 RS: 426\n",
      "Test: 0.9736842105263158 Train: 0.9560439560439561 RS: 428\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 429\n",
      "Test: 0.9736842105263158 Train: 0.9560439560439561 RS: 430\n",
      "Test: 0.956140350877193 Train: 0.9494505494505494 RS: 433\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 435\n",
      "Test: 0.9824561403508771 Train: 0.9494505494505494 RS: 436\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 439\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 446\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 449\n",
      "Test: 0.9736842105263158 Train: 0.9560439560439561 RS: 450\n",
      "Test: 0.9736842105263158 Train: 0.9538461538461539 RS: 456\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 457\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 462\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 465\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 467\n",
      "Test: 0.9736842105263158 Train: 0.9472527472527472 RS: 468\n",
      "Test: 0.9824561403508771 Train: 0.9516483516483516 RS: 470\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 471\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 472\n",
      "Test: 0.9736842105263158 Train: 0.9472527472527472 RS: 473\n",
      "Test: 0.956140350877193 Train: 0.9516483516483516 RS: 474\n",
      "Test: 0.9736842105263158 Train: 0.9494505494505494 RS: 477\n",
      "Test: 0.9649122807017544 Train: 0.9494505494505494 RS: 479\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 482\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 483\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 485\n",
      "Test: 0.956140350877193 Train: 0.9494505494505494 RS: 489\n",
      "Test: 0.9912280701754386 Train: 0.9516483516483516 RS: 490\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 494\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 497\n",
      "Test: 0.9736842105263158 Train: 0.9494505494505494 RS: 498\n",
      "Test: 0.9912280701754386 Train: 0.9494505494505494 RS: 499\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 502\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 503\n",
      "Test: 0.9736842105263158 Train: 0.9494505494505494 RS: 505\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 509\n",
      "Test: 0.9649122807017544 Train: 0.9582417582417583 RS: 520\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 521\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 524\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 525\n",
      "Test: 0.9649122807017544 Train: 0.9582417582417583 RS: 528\n",
      "Test: 0.9736842105263158 Train: 0.9582417582417583 RS: 529\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 530\n",
      "Test: 0.956140350877193 Train: 0.9560439560439561 RS: 531\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 532\n",
      "Test: 0.9824561403508771 Train: 0.9538461538461539 RS: 533\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 535\n",
      "Test: 0.9649122807017544 Train: 0.9494505494505494 RS: 537\n",
      "Test: 0.9736842105263158 Train: 0.9516483516483516 RS: 544\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 546\n",
      "Test: 0.956140350877193 Train: 0.9494505494505494 RS: 550\n",
      "Test: 0.9824561403508771 Train: 0.9494505494505494 RS: 551\n",
      "Test: 0.956140350877193 Train: 0.9538461538461539 RS: 553\n",
      "Test: 0.9649122807017544 Train: 0.9494505494505494 RS: 559\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 560\n",
      "Test: 0.9649122807017544 Train: 0.9516483516483516 RS: 565\n",
      "Test: 0.9649122807017544 Train: 0.9560439560439561 RS: 566\n",
      "Test: 0.9649122807017544 Train: 0.9538461538461539 RS: 567\n",
      "Test: 0.9649122807017544 Train: 0.9494505494505494 RS: 569\n",
      "The random state for the max test score  of 1.0 is 155  \n",
      "Since test score is greater than train score this model is good\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def determine_RS(features, label):\n",
    "    max_val = 0\n",
    "    random_state = 0\n",
    "    hit = 0 # This flag is fired when the test score is greater than train score\n",
    "    #Since there are 569 records lets try iteration over 735\n",
    "    for i in range(1,570):\n",
    "        X_train,X_test,y_train,y_test = train_test_split(features,\n",
    "                                                        label,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state = i)\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        train_score = model.score(X_train,y_train)\n",
    "        test_score = model.score(X_test,y_test)\n",
    "\n",
    "        if test_score > train_score:\n",
    "            hit = 1\n",
    "            if test_score > max_val:\n",
    "                max_val = test_score\n",
    "                random_state = i\n",
    "            print(\"Test: {} Train: {} RS: {}\".format(test_score,train_score,i))\n",
    "    return [max_val, random_state, hit]\n",
    "\n",
    "max_test_score ,random_state, hit = determine_RS(features, label)\n",
    "\n",
    "print (\"The random state for the max test score  of %r is %r  \" % (max_test_score, random_state))\n",
    "if hit:\n",
    "    print (\"Since test score is greater than train score this model is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-efficients: \n",
      "[[-2.03416178 -0.08872027  0.0610022   0.00316431  0.14727732  0.3828063\n",
      "   0.61944729  0.32993889  0.21921429  0.02407132  0.00655975 -1.19364682\n",
      "   0.01244004  0.08682959  0.01648724 -0.01376002  0.03482887  0.03791766\n",
      "   0.04258301 -0.00761776 -1.19747726  0.30876168  0.12484865  0.02271301\n",
      "   0.27364735  1.06704634  1.51333064  0.63772287  0.6897327   0.10662295]]\n",
      "Intercept: \n",
      "[-0.38952512]\n",
      " Train score 0.9494505494505494 \n",
      " Test score 1.0 \n"
     ]
    }
   ],
   "source": [
    "# Now  Create Train Test Splits with the best random state\n",
    "\n",
    "def apply_best_RS(random_state, features):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train,X_test,y_train,y_test = train_test_split(features,\n",
    "                                                     label,\n",
    "                                                     test_size=0.2,\n",
    "                                                     random_state = random_state)\n",
    "    lrModel = LogisticRegression()\n",
    "\n",
    "    lrModel.fit(X_train,y_train)\n",
    "    #Lets Explore the equation\n",
    "    print(\"Co-efficients: \")\n",
    "    print(lrModel.coef_)\n",
    "    print(\"Intercept: \")\n",
    "    print(lrModel.intercept_)\n",
    "    # Check the quality of the model\n",
    "    # We use accuracy check as a mechanism to check the quality of the model\n",
    "    print ( \" Train score %r \" % lrModel.score(X_train,y_train))\n",
    "    # To ensure our model quality is GOOD, ensure your model performs well with Unknown data\n",
    "    print ( \" Test score %r \" %lrModel.score(X_test,y_test))\n",
    "    return lrModel, X_train, X_test, y_train, y_test\n",
    "\n",
    "lrModel,X_train, X_test,y_train, y_test = apply_best_RS(random_state, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the Quality of Model\n",
    "# 1. Ensure your model is a generalized model\n",
    "# 2. If dataset is balanced, check the accuracy score and compare the same with the CL value\n",
    "#.   If dataset is unbalanced, (Suggestion by Prashant Nair)\n",
    "#.        1. Check the Non-tolerable scenario and get the AREA OF FOCUS.\n",
    "#         2. Based on AREA OF FOCUS, get relevant Precision and Recall Scores\n",
    "#.        3. Take AVG and compare the same with CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.   Since the dataset is unbalanced,\n",
    "#.        0. Get the confusion Matrix and All metric values\n",
    "#.        1. Check the Non-tolerable scenario and get the AREA OF FOCUS.\n",
    "#         2. Based on AREA OF FOCUS, get relevant Precision and Recall Scores\n",
    "#.        3. Take AVG and compare the same with CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-tolerable: Customer who is having Malignant Cancer  shouldnt be listed in Benign Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[347,  10],\n",
       "       [ 13, 199]], dtype=int64)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.        0. Get the confusion Matrix and All metric values\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(label, lrModel.predict(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       357\n",
      "           1       0.95      0.94      0.95       212\n",
      "\n",
      "    accuracy                           0.96       569\n",
      "   macro avg       0.96      0.96      0.96       569\n",
      "weighted avg       0.96      0.96      0.96       569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Arrange BEnign and Malign in Ascening order\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(label, lrModel.predict(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third Quadrant will be our Area of Focus\n",
    "#Area of Focus = Avg(Precision of Benign and Recall of Malign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOF = (0.96 + 0.94) / 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confidence Level = 1 - SL\n",
    "CL = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accept\n"
     ]
    }
   ],
   "source": [
    "#Validate if average of precision and recall is greater than CL for unbalanced support in order to accept model\n",
    "if (AOF  >= CL):\n",
    "    print(\"Accept\")\n",
    "else:\n",
    "    print(\"Reject\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion : Accept the model as computed value is greater than confidence value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Visualization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a binary classification problem Hence we can use logistic regression\n",
    "#Since this is an unbalanced dataset we need to use the PR curve metric and not ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e588678088>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Step1: Extract the prob values for label 1\n",
    "probabilityValues = lrModel.predict_proba(X_test)[:,1]\n",
    "\n",
    "#Step2: Calc AUC\n",
    "precision,recall,threshold = precision_recall_curve(y_test,probabilityValues)\n",
    "auc = auc(recall,precision)\n",
    "\n",
    "#Step3: Plot Skill Line\n",
    "plt.plot([0,1],[0.5,0.5], linestyle='--')\n",
    "\n",
    "#Step4: Plot PR curve\n",
    "plt.plot(recall,precision)\n",
    "\n",
    "\n",
    "#Since the PR curve is above skill line, its a Full skill, so we accept model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "modelKNN = KNeighborsClassifier(n_neighbors=5)\n",
    "modelKNN.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9318681318681319"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelKNN.score(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelKNN.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e5889b7948>]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAROElEQVR4nO3df5BdZX3H8fc3CSGg/JKsDpBfBIMQrRa4k8pgBUE0UEjG32EGKxZJsQO29ifWjtrY1qn9Qcc2VWOHojgSkHE0UDQdAQelRLIRiCYhsEQg2zCyIMQqSgh8+8e9yM3u3b0nyb17s0/er5k7uec8z57zffbe/eTsc87dE5mJJGnim9TrAiRJnWGgS1IhDHRJKoSBLkmFMNAlqRBTerXj6dOn55w5c3q1e0makNatW/d4Zva1autZoM+ZM4f+/v5e7V6SJqSIeHi0NqdcJKkQBrokFcJAl6RCGOiSVAgDXZIK0TbQI+KqiHgsIn40SntExGciYiAi1kfEyZ0vU5LUTpUj9KuBhWO0nwPMazyWAp/d+7IkSbur7XXomXl7RMwZo8ti4EtZ/zu8ayLi8Ig4KjMf7VCNu3r4Tnjw1q5sWtJ+YsqBMPdNcPRJMKmcmedOfLDoGGBr0/JgY92IQI+IpdSP4pk1a9ae7W3wLrj9H/bsayUJgIRbPwmHHA0n/A6ceB7MPg0mH9DrwvZKJwI9WqxredeMzFwBrACo1Wp7dmeN0/6w/pCkPfXLJ+H+1bDpRrj7y7D2CzDtcHjVOXDCeXDcmTD14F5Xuds6EeiDwMym5RnAtg5sV5K646Aj4HVL6o8dT8ODt8Cmm2DzzXDvtTDlIHjlWXDi+XD8W+v9J4BOBPoq4LKIWAn8FrC9a/PnktRpUw+uB/eJ58Nzz8JD34P7boL7/qv+76QpMOcN9SP3E86DQ4/qdcWjinb3FI2Ia4EzgOnAT4CPAwcAZObnIiKAf6N+JczTwPszs+1f3arVaukf55K0z3r+edj2g/q0zH03wRMD9fXH1F78D+DI48a9rIhYl5m1lm29ukm0gS5pwsiEoc2NcL8RHr23vr7vxPoJ1RPOg6NeB9HqlGJnGeiS1ElPPVKfktl0EzzyP5DPw2GzXrxiZtapMGlyV3ZtoEtSt/zicdj8zfq0zIO3wXPPwMFHNq6YOR/mngEHTOvY7gx0SRoPz/wfDHy7fuT+wH/DMz+DqS+FV765Puc+72yYdthe7WKsQO/ZHYskqTgHHgKvflv9sfMZ+PHt9Xn3zTfDxq/DpANg7un1z9Ic+8aO795Al6RumHJg/Yh83tnw/JWw9a76tMymG+sfbOrGLruyVUnSiyZNhtmn1h9v+Zv6VTNdYKBL0niK6NrljeX8mTFJ2s8Z6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVolKgR8TCiNgcEQMRcUWL9tkRcUtErI+I70TEjM6XKkkaS9tAj4jJwHLgHGA+cEFEzB/W7R+BL2Xma4FlwKc6XagkaWxVjtAXAAOZuSUzdwArgcXD+swHbmk8v61FuySpy6oE+jHA1qblwca6ZvcC72g8fxtwSEQcOXxDEbE0Ivojon9oaGhP6pUkjaJKoEeLdTls+U+B0yPibuB04H+BnSO+KHNFZtYys9bX17fbxUqSRjelQp9BYGbT8gxgW3OHzNwGvB0gIl4KvCMzt3eqSElSe1WO0NcC8yLi2IiYCiwBVjV3iIjpEfHCtj4CXNXZMiVJ7bQN9MzcCVwGrAY2Addn5oaIWBYRixrdzgA2R8T9wCuAv+1SvZKkUUTm8Onw8VGr1bK/v78n+5akiSoi1mVmrVWbnxSVpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhagU6BGxMCI2R8RARFzRon1WRNwWEXdHxPqIOLfzpUqSxtI20CNiMrAcOAeYD1wQEfOHdfsr4PrMPAlYAvx7pwuVJI2tyhH6AmAgM7dk5g5gJbB4WJ8EDm08PwzY1rkSJUlVVAn0Y4CtTcuDjXXNPgFcGBGDwM3A5a02FBFLI6I/IvqHhob2oFxJ0miqBHq0WJfDli8Ars7MGcC5wDURMWLbmbkiM2uZWevr69v9aiVJo6oS6IPAzKblGYycUrkYuB4gM+8EpgHTO1GgJKmaKoG+FpgXEcdGxFTqJz1XDevzCHAWQEScSD3QnVORpHHUNtAzcydwGbAa2ET9apYNEbEsIhY1uv0JcElE3AtcC1yUmcOnZSRJXTSlSqfMvJn6yc7mdR9rer4ROK2zpUmSdoefFJWkQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRCVAj0iFkbE5ogYiIgrWrRfGRH3NB73R8RTnS9VkjSWKe06RMRkYDlwNjAIrI2IVZm58YU+mfnhpv6XAyd1oVZJ0hiqHKEvAAYyc0tm7gBWAovH6H8BcG0nipMkVVcl0I8BtjYtDzbWjRARs4FjgVtHaV8aEf0R0T80NLS7tUqSxlAl0KPFuhyl7xLghsx8rlVjZq7IzFpm1vr6+qrWKEmqoEqgDwIzm5ZnANtG6bsEp1skqSeqBPpaYF5EHBsRU6mH9qrhnSLiVcARwJ2dLVGSVEXbQM/MncBlwGpgE3B9Zm6IiGURsaip6wXAyswcbTpGktRFbS9bBMjMm4Gbh6372LDlT3SuLEnS7vKTopJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQlQI9IhZGxOaIGIiIK0bp8+6I2BgRGyLiK50tU5LUzpR2HSJiMrAcOBsYBNZGxKrM3NjUZx7wEeC0zHwyIl7erYIlSa1VOUJfAAxk5pbM3AGsBBYP63MJsDwznwTIzMc6W6YkqZ0qgX4MsLVpebCxrtnxwPERcUdErImIha02FBFLI6I/IvqHhob2rGJJUktVAj1arMthy1OAecAZwAXAf0TE4SO+KHNFZtYys9bX17e7tUqSxlAl0AeBmU3LM4BtLfp8IzOfzcwfA5upB7wkaZxUCfS1wLyIODYipgJLgFXD+nwdeBNAREynPgWzpZOFSpLG1jbQM3MncBmwGtgEXJ+ZGyJiWUQsanRbDTwRERuB24A/y8wnulW0JGmkyBw+HT4+arVa9vf392TfkjRRRcS6zKy1avOTopJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFaJSoEfEwojYHBEDEXFFi/aLImIoIu5pPD7Q+VIlSWOZ0q5DREwGlgNnA4PA2ohYlZkbh3W9LjMv60KNkqQK2gY6sAAYyMwtABGxElgMDA/0cfOez985Yt15rz2K9546h1/ueI6L/vOuEe3vPGUG76rN5Ke/2MEHv7xuRPuFr5/N+a87mm1P/ZIPX3fPiPZLfnsub57/Ch4c+jl/+bUfjmi//Mx5vGHedDZs286yG0d+a/584as4ZfbLWPfwT/n0tzaPaP/Y+fN59dGH8b0HHudfb31gRPvfvf03OK7vpXx740/4wne3jGi/8j2/ydGHH8SN927jy2seHtH+2QtP4WUvmcpX+7dyw7rBEe1Xv38BB02dzDV3PsRN6x8d0X7d758KwIrbH+SWTY/t0jbtgMl88fcWAPCZWx7gjoHHd2k/4uCpfO69pwDw99+6jx88/OQu7UcdNo1/WXISAH994wY2bvvZLu1z+17Cp97+WgA+8rX1bBn6xS7t848+lI+f/2oA/mjl3Ty6/Ve7tJ88+wj+YuEJAFx6zTqefHrHLu2nvXI6HzprHgDvu+oufvXsc7u0n3Xiy1n6xuMA33u+9zrz3nthTJ1WZcrlGGBr0/JgY91w74iI9RFxQ0TMbLWhiFgaEf0R0T80NLQH5UqSRhOZOXaHiHcBb83MDzSW3wssyMzLm/ocCfw8M5+JiEuBd2fmmWNtt1arZX9//14PQJL2JxGxLjNrrdqqHKEPAs1H3DOAbc0dMvOJzHymsfgF4JQ9KVSStOeqBPpaYF5EHBsRU4ElwKrmDhFxVNPiImBT50qUJFXR9qRoZu6MiMuA1cBk4KrM3BARy4D+zFwFfCgiFgE7gZ8CF3WxZklSC23n0LvFOXRJ2n17O4cuSZoADHRJKoSBLkmFMNAlqRA9OykaEUPAyM8JVzMdeLxtr7I45v2DY94/7M2YZ2dmX6uGngX63oiI/tHO8pbKMe8fHPP+oVtjdspFkgphoEtSISZqoK/odQE94Jj3D455/9CVMU/IOXRJ0kgT9QhdkjSMgS5JhdinA73CzakPjIjrGu3fj4g5419lZ1UY8x9HxMbG3aFuiYjZvaizk9qNuanfOyMiI2LCX+JWZcwR8e7Ga70hIr4y3jV2WoX39qyIuC0i7m68v8/tRZ2dEhFXRcRjEfGjUdojIj7T+H6sj4iT93qnmblPPqj/qd4HgbnAVOBeYP6wPn8AfK7xfAn1G1X3vPYuj/lNwMGN5x/cH8bc6HcIcDuwBqj1uu5xeJ3nAXcDRzSWX97rusdhzCuADzaezwce6nXdeznmNwInAz8apf1c4JtAAK8Hvr+3+9yXj9B/fXPqzNwBvHBz6maLgS82nt8AnBURMY41dlrbMWfmbZn5dGNxDfU7SE1kVV5ngE8CnwZ+1aJtoqky5kuA5Zn5JEBmPsbEVmXMCRzaeH4Yw+6MNtFk5u3U7w8xmsXAl7JuDXD4sJsF7bZ9OdCr3Jz6130ycyewHThyXKrrjqo35H7BxdT/h5/I2o45Ik4CZmbmTeNZWBdVeZ2PB46PiDsiYk1ELBy36rqjypg/AVwYEYPAzcDllG13f97banvHoh5qdaQ9/BrLKn0mksrjiYgLgRpwelcr6r4xxxwRk4ArKesuWFVe5ynUp13OoP5b2Hcj4jWZ+VSXa+uWKmO+ALg6M/8pIk4FrmmM+fnul9cTHc+vffkIve3NqZv7RMQU6r+mjfUrzr6uypiJiDcDHwUW5Ys3556o2o35EOA1wHci4iHqc42rJviJ0arv7W9k5rOZ+WNgM/WAn6iqjPli4HqAzLwTmEb9j1iVqtLP++7YlwO97c2pG8vvazx/J3BrNs42TFBVbsh9EvB56mE+0edVoc2YM3N7Zk7PzDmZOYf6eYNFmTmR719Y5b39deonwImI6dSnYLaMa5WdVWXMjwBnAUTEidQDfWhcqxxfq4DfbVzt8npge2Y+uldb7PWZ4DZnic8F7qd+dvyjjXXLqP9AQ/0F/yowANwFzO11zeMw5m8DPwHuaTxW9brmbo95WN/vMMGvcqn4Ogfwz8BG4IfAkl7XPA5jng/cQf0KmHuAt/S65r0c77XAo8Cz1I/GLwYuBS5teo2XN74fP+zE+9qP/ktSIfblKRdJ0m4w0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1Ih/h8ZYVqgmJKalgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Step1: Extract the prob values for label 1\n",
    "probabilityValues = modelKNN.predict_proba(X_test)[:,1]\n",
    "\n",
    "#Step2: Calc AUC\n",
    "precision,recall,threshold = precision_recall_curve(y_test,probabilityValues)\n",
    "auc = auc(recall,precision)\n",
    "\n",
    "#Step3: Plot Skill Line\n",
    "plt.plot([0,1],[0.5,0.5], linestyle='--')\n",
    "\n",
    "#Step4: Plot PR curve\n",
    "plt.plot(recall,precision)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion : Accept the model as PR curve value is greater than skill line for both LogisticRegression and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
